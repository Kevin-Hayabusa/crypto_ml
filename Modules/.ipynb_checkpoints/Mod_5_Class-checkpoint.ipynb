{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------ Module 5: Machine Learning Model ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What: Create class to coordinate model usage\n",
    "- When: 05 August 2022   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Define Parameters:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data (if any)\n",
    "Data_Path = './data/'\n",
    "Data_Name = 'BTCUSDT_1MINUTE.feather'\n",
    "\n",
    "# Path to model (Required)\n",
    "Model_Path = './Models/'\n",
    "Model_Name = 'BTC_20210601_20211001'\n",
    "\n",
    "# Path to class (Required)\n",
    "Class_Path = './Classes/'\n",
    "Class_Name = 'Indicator_Class.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load some key libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed successfully. Sat 06 Aug @ 11:04:37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import date,datetime\n",
    "\n",
    "# Function to keep track of code execution.\n",
    "def Keep_Track():\n",
    "    print(\"Executed successfully. \" + datetime.now().strftime(\"%a %d %b @ %H:%M:%S\") + \"\\n\")\n",
    "    \n",
    "Keep_Track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build a class which will coordinate model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed successfully. Sat 06 Aug @ 11:04:47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Build a class that will coordinate the usage of the model.\n",
    "\n",
    "class Indicator_Model():\n",
    "    \n",
    "    # Identify the source of the data and the trained model\n",
    "    def __init__(self, Data_Path, Model_Path, Symbol, ModelName):\n",
    "        \n",
    "        # Path to the data\n",
    "        self.Data_Path = Data_Path\n",
    "        \n",
    "        # Path to the model\n",
    "        self.Model_Path = Model_Path\n",
    "        \n",
    "        # Data file name\n",
    "        self.Symbol = Symbol\n",
    "        \n",
    "        # Saved model name\n",
    "        self.ModelName = ModelName\n",
    "        \n",
    "        # Model and data status\n",
    "        self.DataLoaded = False\n",
    "        self.DataError = True\n",
    "        self.ModelLoaded = False\n",
    "        self.Anyforecast = False\n",
    "        \n",
    "        self.DataLoadedWhen = '-'\n",
    "        self.ModelLoadedWhen = '-'\n",
    "        self.ModelExecutedWhen = '-'\n",
    "        self.ForecastWhen = '-'\n",
    "        \n",
    "        self.FileName = ['None', 'None']\n",
    "        \n",
    "        self.__Lag = [60,180]\n",
    "        self.__factor_functions=[self.__Closing, self.__Volume, self.__cal_close_avg,self.__cal_bk_return,\n",
    "                          self.__cal_high_low,self.__cal_max_mean,self.__cal_min_mean, self.__cal_volume_avg,\n",
    "                          self.__cal_max_mean_volume,self.__cal_min_mean_volume,self.__cal_std, \n",
    "                          self.__r_zscore, self.__Parkinson]\n",
    "\n",
    "        self.get_data()\n",
    "        self.get_model()\n",
    "        \n",
    "        \n",
    "    # Function to provide some details on the state of the class.    \n",
    "    def get_info(self):\n",
    "        print('* Data Loaded :', self.DataLoaded)\n",
    "        print('  - Symbol    :', self.FileName[0])\n",
    "        print('  - Type      :', self.FileName[-1])\n",
    "        print('  - Data Error:', self.DataError)\n",
    "        if self.DataLoaded:\n",
    "            print('  - Loaded @  :', self.DataLoadedWhen)\n",
    "            print('  - Records   :', len(self.data))\n",
    "            print('  - Max date  :', self.Max_Date)\n",
    "            print('  - Min date  :', self.Min_Date)\n",
    "        \n",
    "        print('* Model Loaded:', self.ModelLoaded)\n",
    "        print('  - Model name:', self.ModelName)\n",
    "        if self.ModelLoaded:\n",
    "            print('  - Loaded @  :', self.ModelLoadedWhen)\n",
    "        \n",
    "        print('* Forecast(s) :', self.Anyforecast)\n",
    "        if self.Anyforecast:\n",
    "            print('  - Forecast @ :', self.ForecastWhen)\n",
    "        \n",
    "        print('')\n",
    "        self.__Keep_Track()\n",
    "        \n",
    "    # Keep track of code execution    \n",
    "    def __Keep_Track(self):\n",
    "            print(\"  Executed successfully. \" + datetime.now().strftime(\"%a %d %b @ %H:%M:%S\") + \"\\n\")\n",
    "    \n",
    "    # Check the data contains the columns needed\n",
    "    def __DataCheck(self, Data):    \n",
    "        return all(elem in Data  for elem in ['Open Time', 'Close', 'Volume','High', 'Low'])\n",
    "    \n",
    "    \n",
    "    # Load the data -- allow for various file types.\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            \n",
    "            if self.Symbol == None:\n",
    "                # In this case we want to use a dataframe directly (i.e. not load it from file)\n",
    "                df = self.Data_Path\n",
    "                if not self.__DataCheck(df):\n",
    "                    self.DataError = True\n",
    "                    print('* File error - fields missing')\n",
    "                \n",
    "                else:\n",
    "                    self.DataError = False\n",
    "                    self.Max_Date = df['Open Time'].max()\n",
    "                    self.Min_Date = df['Open Time'].min()\n",
    "                    df = df.set_index('Open Time')\n",
    "                    self.DataLoadedWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "                    self.DataLoaded = True \n",
    "                    \n",
    "                \n",
    "                self.FileName = ['User', 'None']    \n",
    "                df['Ticker'] = 'SelfLoad'\n",
    "                \n",
    "                \n",
    "            elif self.Data_Path == 'Binance': # Load direct from Binance\n",
    "                df = self.get_binance(self.Symbol)\n",
    "                self.DataError = False\n",
    "                self.Max_Date = df['Open Time'].max()\n",
    "                self.Min_Date = df['Open Time'].min()\n",
    "                df = df.set_index('Open Time')\n",
    "                self.DataLoadedWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "                self.DataLoaded = True \n",
    "                self.FileName = ['Binance', 'None']    \n",
    "                df['Ticker'] = self.Symbol \n",
    "                print('* Binance Data     :', self.Symbol)\n",
    "                \n",
    "            else:\n",
    "                self.FileName = self.Symbol.split('.')\n",
    "                self.extension = self.FileName[1]\n",
    "\n",
    "                if self.extension=='feather':\n",
    "                    print('* Feather data file:', self.Symbol)\n",
    "                    df = pd.read_feather(os.path.join(self.Data_Path, f'{self.Symbol}'))\n",
    "                    if not self.__DataCheck(df):\n",
    "                        self.DataError = True\n",
    "                        print('* File error - fields missing')\n",
    "                    else:\n",
    "                        self.DataError = False\n",
    "                        self.Max_Date = df['Open Time'].max()\n",
    "                        self.Min_Date = df['Open Time'].min()\n",
    "                        df = df.set_index('Open Time')\n",
    "                        self.DataLoadedWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "                        self.DataLoaded = True                \n",
    "                elif self.extension =='csv':\n",
    "                    print('* CSV data file    :', self.Symbol)\n",
    "                    df = pd.read_csv(os.path.join(path, f'{self.Symbol}'))\n",
    "                    if not self.__DataCheck(df):\n",
    "                        self.DataError = True\n",
    "                        print('* File error - fields missing')\n",
    "                    else:\n",
    "                        self.DataError = False\n",
    "                        self.Max_Date = df['Open Time'].max()\n",
    "                        self.Min_Date = df['Open Time'].min()\n",
    "                        df = df.set_index('Open Time')\n",
    "                        self.DataLoadedWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "                        self.DataLoaded = True\n",
    "                else:\n",
    "                    print('type not supported')\n",
    "                \n",
    "                df['Ticker'] =  self.FileName[0]\n",
    "                \n",
    "                        \n",
    "            \n",
    "            self.data = df\n",
    "        \n",
    "        except:\n",
    "            print('  !!! Error loading data !!!')\n",
    "              \n",
    "   \n",
    "    # Load the Model\n",
    "    def get_model(self):\n",
    "        try:\n",
    "            self.Model = pickle.load(open(os.path.join(self.Model_Path, f'{self.ModelName}.pkl'),'rb'))\n",
    "            print('* Model loaded     :', self.ModelName)\n",
    "            self.ModelLoaded = True\n",
    "            self.ModelLoadedWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "        except:\n",
    "            print('* Error loading model!')\n",
    "            \n",
    "    def __cal_close_avg(self, unused):\n",
    "        # Current close/avg\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds = (self.data_run.Close/(self.data_run.Close.rolling(i).mean()))\n",
    "            d[f'CurCloseDivAvg_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_volume_avg(self, unused):\n",
    "        # Current volume/avg\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds = (self.data_run.Volume/(self.data_run.Volume.rolling(i).mean()))\n",
    "            d[f'CurVolumeDivAvg_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_bk_return(self, unused):\n",
    "        # Period return up to now\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds = self.data_run.Close.pct_change(i)\n",
    "            d[f'BackReturn_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_high_low(self, unused):\n",
    "        #rolling high/rolling low\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.High.rolling(i).max()/self.data_run.Low.rolling(i).min()\n",
    "            d[f'HighDivLow_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_max_mean(self, unused):\n",
    "        #rolling high/rolling mean\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.High.rolling(i).max()/self.data_run.Close.rolling(i).mean()\n",
    "            d[f'MaxDivMean_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_min_mean(self, unused):\n",
    "        #rolling min/rolling mean\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Low.rolling(i).min()/self.data_run.Close.rolling(i).mean()\n",
    "            d[f'MinDivMean_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_max_mean_volume(self, unused):\n",
    "        #rolling high/rolling mean for volume\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Volume.rolling(i).max()/self.data_run.Volume.rolling(i).mean()\n",
    "            d[f'MaxDivMeanVolume_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_min_mean_volume(self, unused):\n",
    "        #rolling min/rolling mean for volume\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Volume.rolling(i).min()/self.data_run.Volume.rolling(i).mean()\n",
    "            d[f'MinDivMeanVolume_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __cal_std(self, unused):\n",
    "        #rolling std \n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Close.rolling(i).std()\n",
    "            d[f'Std_{i}']=ds\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __Closing(self, unused):\n",
    "        #rolling std \n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Close.shift(i)\n",
    "            d[f'Close_{i}']=ds\n",
    "\n",
    "         # Include current by default \n",
    "        d[f'Close_{0}'] = self.data_run.Close\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __Volume(self, unused):\n",
    "        #rolling std \n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=self.data_run.Volume.shift(i)\n",
    "            d[f'Volume_{i}']=ds\n",
    "\n",
    "        # Include current by default     \n",
    "        d[f'Volume_{0}']=self.data_run.Volume   \n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "    def __r_zscore(self, unused):\n",
    "        # z_score Tina\n",
    "        d={}\n",
    "        for i in self.__Lag:\n",
    "            ds=(np.log(self.data_run.Volume) - np.log(self.data_run.Volume).rolling(i).mean())/np.log(self.data_run.Volume).rolling(i).std()\n",
    "            d[f'Z_Volume_{i}']=ds\n",
    "\n",
    "        return pd.concat(d,axis=1)\n",
    "\n",
    "\n",
    "    def __Parkinson(self, unused):\n",
    "        # z_score Tina\n",
    "        d={}\n",
    "\n",
    "        for i in self.__Lag:\n",
    "            ds = np.log(self.data_run.High / self.data_run.Low)**2\n",
    "            d[f'Parkinson_{i}']= 0.5*np.sqrt(ds.rolling(i).sum()) /np.sqrt(np.log(2)*i/(365*1440))\n",
    "\n",
    "        return pd.concat(d,axis=1)                \n",
    "                \n",
    "                \n",
    "    # Generate the features needed by the model with the dataset provided.     \n",
    "    def __generate_factorsMM(self):    \n",
    "        # Generate factors on panel data with rolling window w\n",
    "        l=[]\n",
    "        \n",
    "        for f in self.__factor_functions:\n",
    "            factor = self.data_run.groupby('Ticker').apply(f)\n",
    "            l.append(factor)\n",
    "            \n",
    "        return pd.concat(l,axis=1) \n",
    "    \n",
    "    def Calculate_Indicator(self, records):\n",
    "        try:\n",
    "            if records <=0:\n",
    "                print('! Need to forecast for at least 1 timestamp')\n",
    "\n",
    "            elif  len(self.data.tail(records+180)) < (records+180):\n",
    "                print('! Insufficient data for',records,'forecast(s)')\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Limit the dataset to the range of interest (needed for performance)\n",
    "                # Need at least 180 records\n",
    "                self.data_run = self.data.tail(records+180)\n",
    "                \n",
    "                self.GeneratedFactors = self.__generate_factorsMM()\n",
    "                \n",
    "                # Reset the index\n",
    "                self.GeneratedFactors = self.GeneratedFactors.reset_index()\n",
    "\n",
    "                # Remove all missing values\n",
    "                self.GeneratedFactors = self.GeneratedFactors.dropna()\n",
    "\n",
    "                Temp = self.GeneratedFactors[['Open Time']].copy()\n",
    "\n",
    "                # Drop the date column\n",
    "                Data_Run =  self.GeneratedFactors.drop(columns = ['Open Time']).values\n",
    "\n",
    "                # Run the model    \n",
    "                Prob = self.Model.predict_proba(Data_Run)\n",
    "\n",
    "                Temp['Probability'] = Prob[:,1]\n",
    "                \n",
    "                self.Anyforecast = True\n",
    "                self.ForecastWhen = datetime.now().strftime(\"%a %d %b @ %H:%M:%S\")\n",
    "                Temp['Forecast Time'] = self.ForecastWhen\n",
    "                Temp = Temp.reset_index(drop = True)\n",
    "                return Temp[['Forecast Time', 'Open Time', 'Probability']]\n",
    "\n",
    "        except:\n",
    "                print('* Error in forecast')\n",
    "                if not(self.DataLoaded):\n",
    "                    print(' - Please load data')\n",
    "                if not(self.ModelLoaded):\n",
    "                    print(' - Please load a model')\n",
    "                    \n",
    "                    \n",
    "    def get_binance(self, ticker):\n",
    "        \n",
    "        try:\n",
    "            client = Client('','')\n",
    "\n",
    "            kline_interval ='1MINUTE'\n",
    "\n",
    "            #Query binance API and format returned dictionary into dataframe\n",
    "            raw = client.get_historical_klines(ticker, client.KLINE_INTERVAL_1MINUTE,'181 minutes ago UTC')\n",
    "\n",
    "            # Convert to Dataframe\n",
    "            df = pd.DataFrame(raw)\n",
    "\n",
    "            # Define columns\n",
    "            df.columns=['Open Time','Open','High','Low','Close','Volume','Close Time','Quote Asset Volume','Number of Trades','TB Base Volume','TB Quote Volume','ignore']\n",
    "\n",
    "            # Format\n",
    "            df['Open Time']=pd.to_datetime(df['Open Time']/1000,unit='s')\n",
    "            df['Close Time']=pd.to_datetime(df['Close Time']/1000,unit='s')\n",
    "\n",
    "            # Change object values into numeric \n",
    "            numeric_columns=['Open','High','Low','Close','Volume','Quote Asset Volume','TB Base Volume','TB Quote Volume']\n",
    "            df[numeric_columns]=df[numeric_columns].apply(pd.to_numeric,axis=1)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except:\n",
    "            return 0\n",
    "                \n",
    "                \n",
    "Keep_Track()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialise the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed successfully. Sat 06 Aug @ 11:04:58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat = dill.dumps(Indicator_Model)\n",
    "\n",
    "#open text file\n",
    "text_file = open(Class_Path+Class_Name, \"wb\")\n",
    "# text_file = open(\"Indicator_Class.txt\", \"wb\")\n",
    " \n",
    "#write string to file\n",
    "n = text_file.write(dat)\n",
    " \n",
    "#close file\n",
    "text_file.close()\n",
    "\n",
    "Keep_Track()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
